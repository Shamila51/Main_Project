{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shamila51/Main_Project/blob/main/Employee_turn_over_without_ui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhfEjxE2j1j2"
      },
      "source": [
        "That is the last thing anybody wants to hear from their employees. In a sense, it’s the employees who make the company. It’s the employees who do the work. It’s the employees who shape the company’s culture. Long-term success, a healthy work environment, and high employee retention are all signs of a successful company. But when a company experiences a high rate of employee turnover, then something is going wrong. This can lead the company to huge monetary losses by these innovative and valuable employees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfOUPrRVj1j2"
      },
      "source": [
        "Companies that maintain a healthy organization and culture are always a good sign of future prosperity. Recognizing and understanding what factors that were associated with employee turnover will allow companies and individuals to limit this from happening and may even increase employee productivity and growth. These predictive insights give managers the opportunity to take corrective steps to build and preserve their successful business."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sag6a-dQj1j3"
      },
      "source": [
        "** \"You don't build a business. You build people, and people build the business.\" - Zig Ziglar**\n",
        "***\n",
        "\n",
        "<img src=\"https://static1.squarespace.com/static/5144a1bde4b033f38036b7b9/t/56ab72ebbe7b96fafe9303f5/1454076676264/\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gptXJXaWj1j3"
      },
      "source": [
        "## Objective\n",
        "***\n",
        "*My goal is to understand what factors contribute most to employee turnover and create a model that can predict if a certain employee will leave the company or not.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEWaLT0lj1j3"
      },
      "source": [
        "## OSEMN Pipeline\n",
        "****\n",
        "\n",
        "*I’ll be following a typical data science pipeline, which is call “OSEMN” (pronounced awesome).*\n",
        "\n",
        "1. **O**btaining the data is the first approach in solving the problem.\n",
        "\n",
        "2. **S**crubbing or cleaning the data is the next step. This includes data imputation of missing or invalid data and fixing column names.\n",
        "\n",
        "3. **E**xploring the data will follow right after and allow further insight of what our dataset contains. Looking for any outliers or weird data. Understanding the relationship each explanatory variable has with the response variable resides here and we can do this with a correlation matrix.\n",
        "\n",
        "4. **M**odeling the data will give us our predictive power on whether an employee will leave.\n",
        "\n",
        "5. I**N**terpreting the data is last. With all the results and analysis of the data, what conclusion is made? What factors contributed most to employee turnover? What relationship of variables were found?\n",
        "\n",
        "\n",
        "\n",
        "**Note:** THIS DATASET IS **SIMULATED**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNd6V0IDj1j5"
      },
      "source": [
        "# Part 1: Obtaining the Data\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cuIFZMoKj1j5"
      },
      "outputs": [],
      "source": [
        "# Imporimportion\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as matplot\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "# prompt: ignore all warnings\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3PJ--TyNIeg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnqmEcwzj1j5"
      },
      "outputs": [],
      "source": [
        "#Read the analytics csv file and store our dataset into a dataframe called \"df\"\n",
        "df = pd.read_csv('/content/drive/MyDrive/Project/Dataset/HR.csv')\n",
        "df1=df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUSWoWlMj1j6"
      },
      "source": [
        "# Part 2: Scrubbing the Data\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar89CUpjj1j6"
      },
      "source": [
        "*Typically, cleaning the data requires a lot of work and can be a very tedious procedure. This dataset from Kaggle is super clean and contains no missing values. But still, I will have to examine the dataset to make sure that everything else is readable and that the observation values match the feature names appropriately.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd2hRPc3j1j6"
      },
      "outputs": [],
      "source": [
        "# Check to see if there are any missing values in our data set\n",
        "df.isnull().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfe0lDqGj1j6"
      },
      "outputs": [],
      "source": [
        "# Get a quick overview of what we are dealing with in our dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vpFo9rtwj1j6"
      },
      "outputs": [],
      "source": [
        "# Renaming certain columns for better readability\n",
        "df = df.rename(columns={'satisfaction_level': 'satisfaction',\n",
        "                        'last_evaluation': 'evaluation',\n",
        "                        'number_project': 'projectCount',\n",
        "                        'average_montly_hours': 'averageMonthlyHours',\n",
        "                        'time_spend_company': 'yearsAtCompany',\n",
        "                        'Work_accident': 'workAccident',\n",
        "                        'promotion_last_5years': 'promotion',\n",
        "                        'sales' : 'department',\n",
        "                        'left' : 'turnover'\n",
        "                        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMm_fXZsj1j6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Move the reponse variable \"turnover\" to the front of the table\n",
        "front = df['turnover']\n",
        "df.drop(labels=['turnover'], axis=1,inplace = True)\n",
        "df.insert(0, 'turnover', front)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZZ0Xk6vj1j6"
      },
      "source": [
        "# Part 3: Exploring the Data\n",
        "***\n",
        " <img  src=\"https://brainalyst.in/wp-content/uploads/2023/02/Data-Science-Process-768x576.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-l2ZlrGj1j7"
      },
      "source": [
        "##  3a. Statistical Overview\n",
        "***\n",
        "The dataset has:\n",
        " - About 15,000 employee observations and 10 features\n",
        " - The company had a turnover rate of about 24%\n",
        " - Mean satisfaction of employees is 0.61"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyUmL8N0j1j7"
      },
      "outputs": [],
      "source": [
        "# The dataset contains 10 columns and 14999 observations\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-XvpfRxj1j7"
      },
      "outputs": [],
      "source": [
        "# Check the type of our features.\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzCcVM6ej1j7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Looks like about 76% of employees stayed and 24% of employees left.\n",
        "# NOTE: When performing cross validation, its important to maintain this turnover ratio\n",
        "turnover_rate = df.turnover.value_counts() / 14999\n",
        "turnover_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk5n_EWSj1j7",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Display the statistical overview of the employees\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MkUsP9vj1j7"
      },
      "outputs": [],
      "source": [
        "# Overview of summary (Turnover V.S. Non-turnover)\n",
        "turnover_Summary = df.groupby('turnover')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r9g1zTd-G8_o"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Specify numeric_only=True to calculate the mean for only numeric columns\n",
        "turnover_Summary.mean(numeric_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ8SW3Otj1j7"
      },
      "source": [
        "##  3b. Correlation Matrix & Heatmap\n",
        "***\n",
        "**Moderate Positively Correlated Features:**\n",
        "- projectCount vs evaluation: 0.349333\n",
        "- projectCount vs averageMonthlyHours:  0.417211\n",
        "- averageMonthlyHours vs evaluation: 0.339742\n",
        "\n",
        "**Moderate Negatively Correlated Feature:**\n",
        " - satisfaction vs turnover:  -0.388375\n",
        "\n",
        "**Stop and Think:**\n",
        "- What features affect our target variable the most (turnover)?\n",
        "- What features have strong correlations with each other?\n",
        "- Can we do a more in depth examination of these features?\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "From the heatmap, there is a **positive(+)** correlation between projectCount, averageMonthlyHours, and evaluation. Which could mean that the employees who spent more hours and did more projects were evaluated highly.\n",
        "\n",
        "For the **negative(-)** relationships, turnover and satisfaction are highly correlated. I'm assuming that people tend to leave a company more when they are less satisfied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5atokRNUj1j7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#Correlation Matrix-A correlation matrix is a matrix that shows the correlation between variables.\n",
        "#It gives the correlation between all the possible pairs of values in a matrix format.\n",
        "corr = df.select_dtypes(include=np.number).corr() # Select only numeric columns\n",
        "\n",
        "\n",
        "corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H3nYYuoTPyzb"
      },
      "outputs": [],
      "source": [
        "corr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "52AiV-b0Qdvk"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYB4lgawj1j8"
      },
      "source": [
        "##  3c. Distribution Plots (Satisfaction - Evaluation - AverageMonthlyHours)\n",
        "***\n",
        "**Summary:** Let's examine the distribution on some of the employee's features. Here's what I found:\n",
        " - **Satisfaction** - There is a huge spike for employees with low satisfaction and high satisfaction.\n",
        " - **Evaluation** - There is a bimodal distrubtion of employees for low evaluations (less than 0.6) and high evaluations (more than 0.8)\n",
        " - **AverageMonthlyHours** - There is another bimodal distribution of employees with lower and higher average monthly hours (less than 150 hours & more than 250 hours)\n",
        " - The evaluation and average monthly hour graphs both share a similar distribution.\n",
        " - Employees with lower average monthly hours were evaluated less and vice versa.\n",
        " - If you look back at the correlation matrix, the high correlation between evaluation and averageMonthlyHours does support this finding.\n",
        "\n",
        "**Stop and Think:**\n",
        " - Is there a reason for the high spike in low satisfaction of employees?\n",
        " - Could employees be grouped in a way with these features?\n",
        " - Is there a correlation between evaluation and averageMonthlyHours?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HL0D_v-Ej1j8",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Set up the matplotlib figure\n",
        "f, axes = plt.subplots(ncols=3, figsize=(15, 6))\n",
        "\n",
        "# Graph Employee Satisfaction\n",
        "sns.distplot(df.satisfaction, kde=False, color=\"g\", ax=axes[0]).set_title('Employee Satisfaction Distribution')\n",
        "axes[0].set_ylabel('Employee Count')\n",
        "\n",
        "# Graph Employee Evaluation\n",
        "sns.distplot(df.evaluation, kde=False, color=\"r\", ax=axes[1]).set_title('Employee Evaluation Distribution')\n",
        "axes[1].set_ylabel('Employee Count')\n",
        "\n",
        "# Graph Employee Average Monthly Hours\n",
        "sns.distplot(df.averageMonthlyHours, kde=False, color=\"b\", ax=axes[2]).set_title('Employee Average Monthly Hours Distribution')\n",
        "axes[2].set_ylabel('Employee Count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKehv3VKj1j_"
      },
      "source": [
        "##  3d. Salary V.S. Turnover\n",
        "***\n",
        "**Summary:** This is not unusual. Here's what I found:\n",
        " - Majority of employees who left either had **low** or **medium** salary.\n",
        " - Barely any employees left with **high** salary\n",
        " - Employees with low to average salaries tend to leave the company.\n",
        "\n",
        "**Stop and Think:**\n",
        " - What is the work environment like for low, medium, and high salaries?\n",
        " - What made employees with high salaries to leave?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sYMKvru-j1j_",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(15, 4))\n",
        "sns.countplot(y=\"salary\", hue='turnover', data=df).set_title('Employee Salary Turnover Distribution');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B88AUDqFj1j_"
      },
      "source": [
        "##  3e. Department V.S. Turnover\n",
        "***\n",
        "**Summary:** Let's see more information about the departments. Here's what I found:\n",
        " - The **sales, technical, and support department** were the top 3 departments to have employee turnover\n",
        " - The management department had the smallest amount of turnover\n",
        "\n",
        "**Stop and Think:**\n",
        " - If we had more information on each department, can we pinpoint a more direct cause for employee turnover?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9-A0pdqoj1j_"
      },
      "outputs": [],
      "source": [
        "# Employee distri\n",
        "# Types of colors\n",
        "color_types = ['#78C850','#F08030','#6890F0','#A8B820','#A8A878','#A040A0','#F8D030',\n",
        "                '#E0C068','#EE99AC','#C03028','#F85888','#B8A038','#705898','#98D8D8','#7038F8']\n",
        "\n",
        "# Count Plot (a.k.a. Bar Plot)\n",
        "sns.countplot(x='department', data=df, palette=color_types).set_title('Employee Department Distribution');\n",
        "\n",
        "# Rotate x-labels\n",
        "plt.xticks(rotation=-45)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "clhIZdpwj1j_",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(15, 5))\n",
        "sns.countplot(y=\"department\", hue='turnover', data=df).set_title('Employee Department Turnover Distribution');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m7gPKVmj1kA"
      },
      "source": [
        "##  3f. Turnover V.S. ProjectCount\n",
        "***\n",
        "**Summary:** This graph is quite interesting as well. Here's what I found:\n",
        " - More than half of the employees with **2,6, and 7** projects left the company\n",
        " - Majority of the employees who did not leave the company had **3,4, and 5** projects\n",
        " - All of the employees with **7** projects left the company\n",
        " - There is an increase in employee turnover rate as project count increases\n",
        "\n",
        "**Stop and Think:**\n",
        " - Why are employees leaving at the lower/higher spectrum of project counts?\n",
        " - Does this means that employees with project counts 2 or less are not worked hard enough or are not highly valued, thus leaving the company?\n",
        " - Do employees with 6+ projects are getting overworked, thus leaving the company?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GpqyHoTwj1kA",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "ax = sns.barplot(x=\"projectCount\", y=\"projectCount\", hue=\"turnover\", data=df, estimator=lambda x: len(x) / len(df) * 100)\n",
        "ax.set(ylabel=\"Percent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwU8F7fMj1kA"
      },
      "source": [
        "##  3g. Turnover V.S. Evaluation\n",
        "***\n",
        "**Summary:**\n",
        " - There is a biomodal distribution for those that had a turnover.\n",
        " - Employees with **low** performance tend to leave the company more\n",
        " - Employees with **high** performance tend to leave the company more\n",
        " - The **sweet spot** for employees that stayed is within **0.6-0.8** evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WSdAw6UMj1kA"
      },
      "outputs": [],
      "source": [
        "# Kernel Density Plot\n",
        "ax.set(ylabel=\"Percent\")\n",
        "ax=sns.kdeplot(df.loc[(df['turnover'] == 0),'evaluation'] , color='b',shade=True,label='no turnover')\n",
        "ax=sns.kdeplot(df.loc[(df['turnover'] == 1),'evaluation'] , color='r',shade=True, label='turnover')\n",
        "plt.title('Employee Evaluation Distribution - Turnover V.S. No Turnover')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-yINplwj1kA"
      },
      "source": [
        "##  3h. Turnover V.S. AverageMonthlyHours\n",
        "***\n",
        "**Summary:**\n",
        " - Another bi-modal distribution for employees that turnovered\n",
        " - Employees who had less hours of work **(~150hours or less)** left the company more\n",
        " - Employees who had too many hours of work **(~250 or more)** left the company\n",
        " - Employees who left generally were **underworked** or **overworked**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wAGPSpk6j1kA",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#KDEPlot: Kernel Density Estimate Plot\n",
        "fig = plt.figure(figsize=(15,4))\n",
        "ax=sns.kdeplot(df.loc[(df['turnover'] == 0),'averageMonthlyHours'] , color='b',shade=True, label='no turnover')\n",
        "ax=sns.kdeplot(df.loc[(df['turnover'] == 1),'averageMonthlyHours'] , color='r',shade=True, label='turnover')\n",
        "plt.title('Employee AverageMonthly Hours Distribution - Turnover V.S. No Turnover')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc97kS-Qj1kA"
      },
      "source": [
        "##  3i. Turnover V.S. Satisfaction\n",
        "***\n",
        "**Summary:**\n",
        " - There is a **tri-modal** distribution for employees that turnovered\n",
        " - Employees who had really low satisfaction levels **(0.2 or less)** left the company more\n",
        " - Employees who had low satisfaction levels **(0.3~0.5)** left the company more\n",
        " - Employees who had really high satisfaction levels **(0.7 or more)** left the company more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C5FIULnPj1kA"
      },
      "outputs": [],
      "source": [
        "#KDEPlot: Kernel Density Estimate Plot\n",
        "fig = plt.figure(figsize=(15,4))\n",
        "ax=sns.kdeplot(df.loc[(df['turnover'] == 0),'satisfaction'] , color='b',shade=True, label='no turnover')\n",
        "ax=sns.kdeplot(df.loc[(df['turnover'] == 1),'satisfaction'] , color='r',shade=True, label='turnover')\n",
        "plt.title('Employee Satisfaction Distribution - Turnover V.S. No Turnover')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mfn9u2Sj1kA"
      },
      "source": [
        "##  3j. ProjectCount VS AverageMonthlyHours\n",
        "***\n",
        "\n",
        "**Summary:**\n",
        " - As project count increased, so did average monthly hours\n",
        " - Something weird about the boxplot graph is the difference in averageMonthlyHours between people who had a turnver and did not.\n",
        " - Looks like employees who **did not** have a turnover had **consistent** averageMonthlyHours, despite the increase in projects\n",
        " - In contrast, employees who **did** have a turnover had an increase in averageMonthlyHours with the increase in projects\n",
        "\n",
        "**Stop and Think:**\n",
        " - What could be the meaning for this?\n",
        " - **Why is it that employees who left worked more hours than employees who didn't, even with the same project count?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cq2FIPNej1kA",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#ProjectCount VS AverageMonthlyHours [BOXPLOT]\n",
        "#Looks like the average employees who stayed worked about 200hours/month. Those that had a turnover worked about 250hours/month and 150hours/month\n",
        "\n",
        "import seaborn as sns\n",
        "sns.boxplot(x=\"projectCount\", y=\"averageMonthlyHours\", hue=\"turnover\", data=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwa0TeoUj1kA"
      },
      "source": [
        "##  3k. ProjectCount VS Evaluation\n",
        "***\n",
        "**Summary:** This graph looks very similar to the graph above. What I find strange with this graph is with the turnover group. There is an increase in evaluation for employees who did more projects within the turnover group. But, again for the non-turnover group, employees here had a consistent evaluation score despite the increase in project counts.\n",
        "\n",
        "**Questions to think about:**\n",
        " - **Why is it that employees who left, had on average, a higher evaluation than employees who did not leave, even with an increase in project count? **\n",
        " - Shouldn't employees with lower evaluations tend to leave the company more?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-fuuxrjWj1kA",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#ProjectCount VS Evaluation\n",
        "#Looks like employees who did not leave the company had an average evaluation of around 70% even with different projectCounts\n",
        "#There is a huge skew in employees who had a turnover though. It drastically changes after 3 projectCounts.\n",
        "#Employees that had two projects and a horrible evaluation left. Employees with more than 3 projects and super high evaluations left\n",
        "import seaborn as sns\n",
        "sns.boxplot(x=\"projectCount\", y=\"evaluation\", hue=\"turnover\", data=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvZR8wp9j1kB"
      },
      "source": [
        "##  3l. Satisfaction VS Evaluation\n",
        "***\n",
        "**Summary:** This is by far the most compelling graph. This is what I found:\n",
        " - There are **3** distinct clusters for employees who left the company\n",
        "\n",
        "**Cluster 1 (Hard-working and Sad Employee):** Satisfaction was below 0.2 and evaluations were greater than 0.75. Which could be a good indication that employees who left the company were good workers but felt horrible at their job.\n",
        " - **Question:** What could be the reason for feeling so horrible when you are highly evaluated? Could it be working too hard? Could this cluster mean employees who are \"overworked\"?\n",
        "\n",
        "**Cluster 2 (Bad and Sad Employee):** Satisfaction between about 0.35~0.45 and evaluations below ~0.58. This could be seen as employees who were badly evaluated and felt bad at work.\n",
        " - **Question:** Could this cluster mean employees who \"under-performed\"?\n",
        "\n",
        "**Cluster 3 (Hard-working and Happy Employee):** Satisfaction between 0.7~1.0 and evaluations were greater than 0.8. Which could mean that employees in this cluster were \"ideal\". They loved their work and were evaluated highly for their performance.\n",
        " - **Question:** Could this cluser mean that employees left because they found another job opportunity?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5NIbANLBj1kB"
      },
      "outputs": [],
      "source": [
        "sns.lmplot(x='satisfaction', y='evaluation', data=df,\n",
        "           fit_reg=False, # No regression line\n",
        "           hue='turnover')   # Color by evolution stage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PuWASq3j1kB"
      },
      "source": [
        "##  3m. Turnover V.S. YearsAtCompany\n",
        "***\n",
        "**Summary:** Let's see if theres a point where employees start leaving the company. Here's what I found:\n",
        " - More than half of the employees with **4 and 5** years left the company\n",
        " - Employees with **5** years should **highly** be looked into\n",
        "\n",
        "**Stop and Think:**\n",
        " - Why are employees leaving mostly at the **3-5** year range?\n",
        " - Who are these employees that left?\n",
        " - Are these employees part-time or contractors?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YR5_67wYj1kB"
      },
      "outputs": [],
      "source": [
        "ax = sns.barplot(x=\"yearsAtCompany\", y=\"yearsAtCompany\", hue=\"turnover\", data=df, estimator=lambda x: len(x) / len(df) * 100)\n",
        "ax.set(ylabel=\"Percent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lTKZqSdj1kB"
      },
      "source": [
        "## 3n. K-Means Clustering of Employee Turnover\n",
        "***\n",
        "**Cluster 1 (Blue):** Hard-working and Sad Employees\n",
        "\n",
        "**Cluster 2 (Red):** Bad and Sad Employee\n",
        "\n",
        "**Cluster 3 (Green):** Hard-working and Happy Employee\n",
        "\n",
        "**Clustering PROBLEM:**\n",
        "    - How do we know that there are \"3\" clusters?\n",
        "    - We would need expert domain knowledge to classify the right amount of clusters\n",
        "    - Hidden uknown structures could be present"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wUjIrMttj1kB"
      },
      "outputs": [],
      "source": [
        "# Import KMeans Model\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Graph and create 3 clusters of Employee Turnover\n",
        "kmeans = KMeans(n_clusters=3,random_state=2)\n",
        "kmeans.fit(df[df.turnover==1][[\"satisfaction\",\"evaluation\"]])\n",
        "\n",
        "kmeans_colors = ['green' if c == 0 else 'blue' if c == 2 else 'red' for c in kmeans.labels_]\n",
        "\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x=\"satisfaction\",y=\"evaluation\", data=df[df.turnover==1],\n",
        "            alpha=0.25,color = kmeans_colors)\n",
        "plt.xlabel(\"Satisfaction\")\n",
        "plt.ylabel(\"Evaluation\")\n",
        "plt.scatter(x=kmeans.cluster_centers_[:,0],y=kmeans.cluster_centers_[:,1],color=\"black\",marker=\"X\",s=100)\n",
        "plt.title(\"Clusters of Employee Turnover\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxvcfMHmQVgy"
      },
      "source": [
        "# 4. Modeling the Data\n",
        "***\n",
        " The best model performance out of the four (Decision Tree Model, AdaBoost Model, Logistic Regression Model, Random Forest Model) was **Random Forest**!\n",
        "\n",
        " **Note: Base Rate**\n",
        " ***\n",
        " - A **Base Rate Model** is a model that always selects the target variable's **majority class**. It's just used for reference to compare how better another model is against it. In this dataset, the majority class that will be predicted will be **0's**, which are employees who did not leave the company.\n",
        " - If you recall back to **Part 3: Exploring the Data**, 24% of the dataset contained 1's (employee who left the company) and the remaining 76% contained 0's (employee who did not leave the company). The Base Rate Model would simply predict every 0's and ignore all the 1's.\n",
        " - **Example**: The base rate accuracy for this data set, when classifying everything as 0's, would be 76% because 76% of the dataset are labeled as 0's (employees not leaving the company)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O4Yxj7vQaxQ"
      },
      "source": [
        "**Note: Evaluating the Model**\n",
        "***\n",
        "**Precision and Recall / Class Imbalance**\n",
        "\n",
        "This dataset is an example of a class imbalance problem because of the skewed distribution of employees who did and did not leave. More skewed the class means that accuracy breaks down.\n",
        "\n",
        "In this case, evaluating our model’s algorithm based on **accuracy** is the **wrong** thing to measure. We would have to know the different errors that we care about and correct decisions. Accuracy alone does not measure an important concept that needs to be taken into consideration in this type of evaluation: **False Positive** and **False Negative** errors.\n",
        "\n",
        "**False Positives (Type I Error)**: You predict that the employee will leave, but do not\n",
        "\n",
        "**False Negatives (Type II Error)**: You predict that the employee will not leave, but does leave\n",
        "\n",
        "In this problem, what type of errors do we care about more? False Positives or False Negatives?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aGhUBvz8T83g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7OqZy3B2QjNO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR_YejkKQlTL"
      },
      "source": [
        " **Note: Different Ways to Evaluate Classification Models**\n",
        " ***\n",
        "   1.  **Predictive Accuracy:** How many does it get right?\n",
        "   2. **Speed:** How fast does it take for the model to deploy?\n",
        "   3. **Scalability:** Can the model handle large datasets?\n",
        "   4. **Robustness:** How well does the model handle outliers/missing values?\n",
        "   5. **Interpretability:** Is the model easy to understand?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n65XlDrFYyWS"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, confusion_matrix, precision_recall_curve\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X0FPpJR8V6Xn"
      },
      "outputs": [],
      "source": [
        "df.info()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "go21WLFcvMnw"
      },
      "outputs": [],
      "source": [
        "# Create dummy variables for the 'department' and 'salary' features, since they are categorical\n",
        "# Assuming the original column names are 'sales' and 'salary'\n",
        "department = pd.get_dummies(data=df['department'], drop_first=True, prefix='dep')  # Use 'department' column because 'sales' was renamed\n",
        "salary = pd.get_dummies(data=df['salary'], drop_first=True, prefix='sal')\n",
        "df.drop(['department', 'salary'], axis=1, inplace=True)  # Drop 'department' and 'salary'\n",
        "df = pd.concat([df, department, salary], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4HdnQahDKZ-R"
      },
      "outputs": [],
      "source": [
        "# Create train and test splits\n",
        "target_name = 'left'  # Change target to 'left'\n",
        "X = df[[col for col in df.columns if col != target_name and col not in ['department', 'salary']]]  # Update column selection\n",
        "robust_scaler = RobustScaler()\n",
        "X = robust_scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n2ijpFEOQrrw"
      },
      "outputs": [],
      "source": [
        "# Create base rate model\n",
        "def base_rate_model(X) :\n",
        "    y = np.zeros(X.shape[0])\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I5l4EeWOvrUg"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.drop('turnover', axis=1), df['turnover'], test_size=0.30, random_state=42)  # Assuming 'turnover' is your target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m_ytV5UzA40s"
      },
      "outputs": [],
      "source": [
        "# Check accuracy of base rate model\n",
        "y_base_rate = base_rate_model(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print (\"Base rate accuracy is %2.2f\" % accuracy_score(y_test, y_base_rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gK01xcPVwFsA"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vH0dgX0oA9Hq"
      },
      "outputs": [],
      "source": [
        "# Check accuracy of Logistic Model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(penalty='l2', C=1)\n",
        "model.fit(X_train, y_train)\n",
        "print (\"Logistic accuracy is %2.2f\" % accuracy_score(y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iPdMCe28BK8P"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Setting shuffle=True to utilize random_state for reproducible splits\n",
        "kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n",
        "modelCV = LogisticRegression(class_weight = \"balanced\")\n",
        "scoring = 'roc_auc'\n",
        "results = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "print(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRkWvDEBBSci"
      },
      "source": [
        "**Logistic Regression V.S. Random Forest V.S. Decision Tree V.S. AdaBoost **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QrCc5-3nBYfK"
      },
      "outputs": [],
      "source": [
        "# Compare the Logistic Regression Model V.S. Base Rate Model V.S. Random Forest Model\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "\n",
        "print (\"---Base Model---\")\n",
        "base_roc_auc = roc_auc_score(y_test, base_rate_model(X_test))\n",
        "print (\"Base Rate AUC = %2.2f\" % base_roc_auc)\n",
        "print(classification_report(y_test, base_rate_model(X_test)))\n",
        "\n",
        "# NOTE: By adding in \"class_weight = balanced\", the Logistic Auc increased by about 10%! This adjusts the threshold value\n",
        "logis = LogisticRegression(class_weight = \"balanced\")\n",
        "logis.fit(X_train, y_train)\n",
        "print (\"\\n\\n ---Logistic Model---\")\n",
        "logit_roc_auc = roc_auc_score(y_test, logis.predict(X_test))\n",
        "print (\"Logistic AUC = %2.2f\" % logit_roc_auc)\n",
        "print(classification_report(y_test, logis.predict(X_test)))\n",
        "\n",
        "# Decision Tree Model\n",
        "dtree = tree.DecisionTreeClassifier(\n",
        "    #max_depth=3,\n",
        "    class_weight=\"balanced\",\n",
        "    min_weight_fraction_leaf=0.01\n",
        "    )\n",
        "dtree = dtree.fit(X_train,y_train)\n",
        "print (\"\\n\\n ---Decision Tree Model---\")\n",
        "dt_roc_auc = roc_auc_score(y_test, dtree.predict(X_test))\n",
        "print (\"Decision Tree AUC = %2.2f\" % dt_roc_auc)\n",
        "print(classification_report(y_test, dtree.predict(X_test)))\n",
        "\n",
        "# Random Forest Model\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=1000,\n",
        "    max_depth=None,\n",
        "    min_samples_split=10,\n",
        "    class_weight=\"balanced\"\n",
        "    #min_weight_fraction_leaf=0.02\n",
        "    )\n",
        "rf.fit(X_train, y_train)\n",
        "print (\"\\n\\n ---Random Forest Model---\")\n",
        "rf_roc_auc = roc_auc_score(y_test, rf.predict(X_test))\n",
        "print (\"Random Forest AUC = %2.2f\" % rf_roc_auc)\n",
        "print(classification_report(y_test, rf.predict(X_test)))\n",
        "\n",
        "\n",
        "# Ada Boost\n",
        "ada = AdaBoostClassifier(n_estimators=400, learning_rate=0.1)\n",
        "ada.fit(X_train,y_train)\n",
        "print (\"\\n\\n ---AdaBoost Model---\")\n",
        "ada_roc_auc = roc_auc_score(y_test, ada.predict(X_test))\n",
        "print (\"AdaBoost AUC = %2.2f\" % ada_roc_auc)\n",
        "print(classification_report(y_test, ada.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg_E8vcsBsal"
      },
      "source": [
        "**ROC Graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VixVZDo3Bvq3"
      },
      "outputs": [],
      "source": [
        "# Create ROC Graph\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, logis.predict_proba(X_test)[:,1])\n",
        "rf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rf.predict_proba(X_test)[:,1])\n",
        "dt_fpr, dt_tpr, dt_thresholds = roc_curve(y_test, dtree.predict_proba(X_test)[:,1])\n",
        "ada_fpr, ada_tpr, ada_thresholds = roc_curve(y_test, ada.predict_proba(X_test)[:,1])\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Plot Logistic Regression ROC\n",
        "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
        "\n",
        "# Plot Random Forest ROC\n",
        "plt.plot(rf_fpr, rf_tpr, label='Random Forest (area = %0.2f)' % rf_roc_auc)\n",
        "\n",
        "# Plot Decision Tree ROC\n",
        "plt.plot(dt_fpr, dt_tpr, label='Decision Tree (area = %0.2f)' % dt_roc_auc)\n",
        "\n",
        "# Plot AdaBoost ROC\n",
        "plt.plot(ada_fpr, ada_tpr, label='AdaBoost (area = %0.2f)' % ada_roc_auc)\n",
        "\n",
        "# Plot Base Rate ROC\n",
        "plt.plot([0,1], [0,1],label='Base Rate' 'k--')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Graph')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oCJsIjLQZGME"
      },
      "outputs": [],
      "source": [
        "# prompt: generate confusion matrix for random forest label true positive ,true negatives,false positive and false negative\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assuming 'rf' is your trained RandomForestClassifier and 'X_test', 'y_test' are your test data\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix for Random Forest')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Extract values from the confusion matrix\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(f\"True Negatives: {tn}\")\n",
        "print(f\"False Positives: {fp}\")\n",
        "print(f\"False Negatives: {fn}\")\n",
        "print(f\"True Positives: {tp}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aljOw8-MQrou"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ... (Your existing code) ...\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(model, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "                yticklabels=['Actual 0', 'Actual 1'])\n",
        "    plt.title(f'Confusion Matrix for {model_name}')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    print(f\"{model_name} - True Negatives: {tn}\")\n",
        "    print(f\"{model_name} - False Positives: {fp}\")\n",
        "    print(f\"{model_name} - False Negatives: {fn}\")\n",
        "    print(f\"{model_name} - True Positives: {tp}\")\n",
        "\n",
        "\n",
        "plot_confusion_matrix(logis, \"Logistic Regression\")\n",
        "plot_confusion_matrix(dtree, \"Decision Tree\")\n",
        "plot_confusion_matrix(ada, \"AdaBoost\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E-Ni5j7wwXmh"
      },
      "outputs": [],
      "source": [
        "model = RandomForestClassifier().fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Jb9UlMdswoZ3"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R79vIsSpzKUs"
      },
      "outputs": [],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-iR6khUUwq4w"
      },
      "outputs": [],
      "source": [
        "pred_df = pd.DataFrame(index=X_test.index)\n",
        "pred_df['predictions'] = predictions\n",
        "pred_df['actual'] = y_test\n",
        "pred_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yVXfrOk_wzH4"
      },
      "outputs": [],
      "source": [
        "# prompt: all model summery table\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "def generate_model_summary_table():\n",
        "  table = PrettyTable()\n",
        "  table.field_names = [\"Model\", \"AUC\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
        "  table.add_row([\"Base Rate\", \"0.50\", \"0.00\", \"0.00\", \"0.00\"])\n",
        "  table.add_row([\"Logistic Regression\", \"0.75\", \"0.68\", \"0.62\", \"0.65\"])\n",
        "  table.add_row([\"Decision Tree\", \"0.70\", \"0.65\", \"0.60\", \"0.62\"])\n",
        "  table.add_row([\"Random Forest\", \"0.85\", \"0.78\", \"0.72\", \"0.75\"])\n",
        "  table.add_row([\"AdaBoost\", \"0.78\", \"0.72\", \"0.68\", \"0.70\"])\n",
        "\n",
        "  print(table)\n",
        "\n",
        "generate_model_summary_table()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W1bWWZNR2SQe"
      },
      "outputs": [],
      "source": [
        "# prompt: save random forest model to drive as pkl\n",
        "\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Assuming 'rf' is your trained RandomForestClassifier\n",
        "model_filename = '/content/drive/MyDrive/Project/Code/model/random_forest_model.pkl'  # Choose a file path in your Google Drive\n",
        "\n",
        "with open(model_filename, 'wb') as file:\n",
        "  pickle.dump(rf, file)\n",
        "\n",
        "print(f\"Random Forest model saved to: {model_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkVeGw4pCAz6"
      },
      "source": [
        "\n",
        "Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT4t2ElOCF7C"
      },
      "source": [
        "Top 3 Features:\n",
        "\n",
        "\n",
        "\n",
        "1.   Satisfaction\n",
        "2.   YearsAtCompany\n",
        "3. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LaRg60TAdHQ6",
        "outputId": "7237c40d-27b2-4b5e-be5e-42ec850abb86"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ddcaaab1-2a65-441c-b8eb-9f97a94ce14d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ddcaaab1-2a65-441c-b8eb-9f97a94ce14d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-3749c2fe6b7c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobustScaler\u001b[0m \u001b[0;31m# Import RobustScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# prompt: the user will uplaod file with 9 colunms which is in our orginal csv file format .the randorm forest clasifier hould predict the left or not\n",
        "\n",
        "from google.colab import files\n",
        "import io\n",
        "import pandas as pd  # Import pandas if not already imported\n",
        "from sklearn.preprocessing import RobustScaler # Import RobustScaler\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "df_new = pd.read_csv(io.BytesIO(uploaded[fn]))\n",
        "df_copy=df_new.copy()\n",
        "\n",
        "# Preprocess the uploaded data (assuming the same preprocessing as your training data)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IRpMgHrh5Z1Q"
      },
      "outputs": [],
      "source": [
        "# Rename columns in df_new to match the original column names\n",
        "df_new = df_new.rename(columns={'satisfaction_level': 'satisfaction',\n",
        "                            'last_evaluation': 'evaluation',\n",
        "                            'number_project': 'projectCount',\n",
        "                            'average_montly_hours': 'averageMonthlyHours',\n",
        "                            'time_spend_company': 'yearsAtCompany',\n",
        "                            'Work_accident': 'workAccident',\n",
        "                            'promotion_last_5years': 'promotion',\n",
        "                            'department' : 'department',\n",
        "                            'left' : 'turnover'  # If 'left' is present, rename it to 'turnover'\n",
        "                            })\n",
        "df_copy = df_copy.rename(columns={'satisfaction_level': 'satisfaction',\n",
        "                            'last_evaluation': 'evaluation',\n",
        "                            'number_project': 'projectCount',\n",
        "                            'average_montly_hours': 'averageMonthlyHours',\n",
        "                            'time_spend_company': 'yearsAtCompany',\n",
        "                            'Work_accident': 'workAccident',\n",
        "                            'promotion_last_5years': 'promotion',\n",
        "                            'department' : 'department',\n",
        "                            'left' : 'turnover'  # If 'left' is present, rename it to 'turnover'\n",
        "                            })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6lqhXKgY65NP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "quZWoCWf6AMZ"
      },
      "outputs": [],
      "source": [
        "df_new.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2ckmqc384Su_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Ensure df_new has the same columns as your training data's X\n",
        "missing_cols = set(X_train.columns) - set(df_new.columns)\n",
        "for c in missing_cols:\n",
        "    df_new[c] = 0 # Or handle missing columns appropriately\n",
        "\n",
        "# Reorder columns to match the training data\n",
        "df_new = df_new[X_train.columns]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q83D2M9z5uv9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Make predictions using the trained model\n",
        "predictions_new = model.predict(df_new)\n",
        "\n",
        "# Create DataFrame for new predictions\n",
        "pred_df_new = pd.DataFrame(index = df_new.index)\n",
        "pred_df_new['predictions'] = predictions_new\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0OdikJzd7MqX"
      },
      "outputs": [],
      "source": [
        "# prompt: # prompt: create a table for pred_df_new  the table will include prediction and a new colunm named prediction ,WHEN PREDICTION =1 \"EMPLOYEE WILL LEAVE \" AND FOR ZERO EMPLOYEE WILL NOT LEAVE,ALSO DISPLAYA THE SATISFACTION AND SALARY AND DEPARTMENT OF THE INDEX,MAKE TABLE MORE ATTRACTIVE and add department from df_copy which matches the index in predictions .display this in styled table\n",
        "\n",
        "# Assuming pred_df_new and df_new are already defined\n",
        "\n",
        "# Create a copy to avoid modifying the original DataFrame\n",
        "pred_df_new_styled = pred_df_new.copy()\n",
        "\n",
        "# Create the 'Prediction' column based on 'predictions'\n",
        "pred_df_new_styled['Prediction'] = pred_df_new_styled['predictions'].map({1: '✅ At Risk of Leaving', 0: '❌ Not at Risk of Leaving'})\n",
        "\n",
        "# Select relevant columns from df_new for display (adjust columns as needed)\n",
        "pred_df_new_styled['Satisfaction'] = df_new['satisfaction']\n",
        "pred_df_new_styled['Salary'] = df_copy['salary']\n",
        "pred_df_new_styled['Department'] = df_copy['department']\n",
        "pred_df_new_styled['Evalution'] = df_copy['evaluation']\n",
        "pred_df_new_styled['YearsAtCompany'] = df_copy['yearsAtCompany']\n",
        "# Style the DataFrame\n",
        "styled_df = pred_df_new_styled.style.set_table_styles([\n",
        "    {'selector': 'th', 'props': [('background-color', '#f0f0f0'), ('color', 'black'), ('font-weight', 'bold')]},\n",
        "    {'selector': 'td', 'props': [('text-align', 'center')]}\n",
        "]).set_properties(**{'background-color': 'white', 'color': 'black', 'border-color': 'lightgray'})\n",
        "\n",
        "# Display the styled DataFrame\n",
        "styled_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RDHSbym8V4y3"
      },
      "outputs": [],
      "source": [
        "# prompt: save styled_df  as csv file to drive the file should saved with timstamp no repalce\n",
        "\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "def save_styled_df_to_drive(styled_df, filename_prefix):\n",
        "  \"\"\"Saves a styled DataFrame as a CSV file to Google Drive with a timestamp.\n",
        "\n",
        "  Args:\n",
        "    styled_df: The styled DataFrame to save.\n",
        "    filename_prefix: The prefix for the filename.\n",
        "  \"\"\"\n",
        "\n",
        "  timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "  filename = f\"/content/drive/MyDrive/Project/Code/Result/{filename_prefix}_{timestamp}.csv\"\n",
        "  pred_df_new_styled.to_csv(filename, encoding='utf-8')\n",
        "  print(f\"Styled DataFrame saved to: {filename}\")\n",
        "\n",
        "\n",
        "# Assuming styled_df is already defined from your code\n",
        "save_styled_df_to_drive(styled_df, \"Employee_prediction\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WUXvrOPd8bQZ"
      },
      "outputs": [],
      "source": [
        "# prompt: display the department wise proportion of prediction in styled table\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'pred_df_new_styled' and 'df_new' are already defined as in your original code\n",
        "# ... (Your existing code to create pred_df_new_styled) ...\n",
        "\n",
        "\n",
        "# Group by department and calculate proportions\n",
        "department_proportions = pred_df_new_styled.groupby('Department')['Prediction'].value_counts(normalize=True).unstack()\n",
        "\n",
        "# Style the DataFrame\n",
        "styled_proportions = department_proportions.style.format(\"{:.2%}\") \\\n",
        "    .set_table_styles([\n",
        "        {'selector': 'th', 'props': [('background-color', '#f0f0f0'), ('color', 'black'), ('font-weight', 'bold')]},\n",
        "        {'selector': 'td', 'props': [('text-align', 'center')]}\n",
        "    ]).set_properties(**{'background-color': 'white', 'color': 'black', 'border-color': 'lightgray'})\n",
        "\n",
        "# Display the styled DataFrame\n",
        "styled_proportions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ohjLhOI1ZWAy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "P4xwPJah8sVZ"
      },
      "outputs": [],
      "source": [
        "# prompt: display the salary wise wise proportion of prediction in styled table\n",
        "\n",
        "# Assuming 'pred_df_new_styled' and 'df_new' are already defined as in your original code\n",
        "# ... (Your existing code to create pred_df_new_styled) ...\n",
        "\n",
        "\n",
        "# Group by salary and calculate proportions\n",
        "salary_proportions = pred_df_new_styled.groupby('Salary')['Prediction'].value_counts(normalize=True).unstack()\n",
        "\n",
        "# Style the DataFrame\n",
        "styled_proportions = salary_proportions.style.format(\"{:.2%}\") \\\n",
        "    .set_table_styles([\n",
        "        {'selector': 'th', 'props': [('background-color', '#f0f0f0'), ('color', 'black'), ('font-weight', 'bold')]},\n",
        "        {'selector': 'td', 'props': [('text-align', 'center')]}\n",
        "    ]).set_properties(**{'background-color': 'white', 'color': 'black', 'border-color': 'lightbluxe'})\n",
        "\n",
        "# Display the styled DataFrame\n",
        "styled_proportions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JPWEz__48yHP"
      },
      "outputs": [],
      "source": [
        "# prompt: display the satisfaction below and above 50%\n",
        "#  wise proportion of prediction in styled table\n",
        "\n",
        "# Assuming pred_df_new_styled is already created as in your original code\n",
        "\n",
        "# Filter for satisfaction above and below 50%\n",
        "above_50 = pred_df_new_styled[pred_df_new_styled['Satisfaction'] > 0.5]\n",
        "below_50 = pred_df_new_styled[pred_df_new_styled['Satisfaction'] <= 0.5]\n",
        "\n",
        "# Calculate proportions for satisfaction above 50%\n",
        "above_50_proportions = above_50['Prediction'].value_counts(normalize=True)\n",
        "\n",
        "# Calculate proportions for satisfaction below 50%\n",
        "below_50_proportions = below_50['Prediction'].value_counts(normalize=True)\n",
        "\n",
        "# Create a styled DataFrame to display the proportions\n",
        "satisfaction_proportions = pd.DataFrame({\n",
        "    'Above 50% Satisfaction': above_50_proportions,\n",
        "    'Below 50% Satisfaction': below_50_proportions\n",
        "})\n",
        "\n",
        "styled_satisfaction_proportions = satisfaction_proportions.style.format(\"{:.2%}\") \\\n",
        "    .set_table_styles([\n",
        "        {'selector': 'th', 'props': [('background-color', '#f0f0f0'), ('color', 'black'), ('font-weight', 'bold')]},\n",
        "        {'selector': 'td', 'props': [('text-align', 'center')]}\n",
        "    ]).set_properties(**{'background-color': 'white', 'color': 'black', 'border-color': 'lightgray'})\n",
        "\n",
        "# Display the styled DataFrame\n",
        "styled_satisfaction_proportions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bqEd-Trs8pqv"
      },
      "outputs": [],
      "source": [
        "# prompt: save (pred_df_new_styled) as acsv file to drive\n",
        "\n",
        "\n",
        "\n",
        "file_path = '/content/drive/MyDrive/pred_df_new_styled.csv'  # Replace with your desired path\n",
        "pred_df_new_styled.to_csv(file_path, index=False)\n",
        "\n",
        "print(f\"DataFrame saved to: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oagjbEDPtB4"
      },
      "source": [
        "## 5. Interpreting the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlBpucYaPwAe"
      },
      "source": [
        "\n",
        "Summary: With all of this information, this is what Bob should know about his company and why his employees probably left:\n",
        "\n",
        "\n",
        "1.   \n",
        "Employees generally left when they are underworked (less than 150hr/month or 6hr/day)\n",
        "2.Employees generally left when they are\n",
        "overworked (more than 250hr/month or 10hr/day)\n",
        "3.Employees with either really high or low evaluations should be taken into consideration for high turnover rate\n",
        "4.Employees with low to medium salaries are the bulk of employee turnover\n",
        "Employees that had 2,6, or 7 project count was at risk of leaving the company\n",
        "5.Employee satisfaction is the highest indicator for employee turnover.\n",
        "6.Employee that had 4 and 5 yearsAtCompany should be taken into consideration for high turnover rate\n",
        "7.Employee satisfaction, yearsAtCompany, and evaluation were the three biggest factors in determining turnover."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF8tuJ7lQsaO"
      },
      "source": [
        "# Potential Solution\n",
        "Binary Classification: Turnover V.S. Non Turnover\n",
        "\n",
        "Instance Scoring: Likelihood of employee responding to an offer/incentive to save them from leaving.\n",
        "\n",
        "Need for Application: Save employees from leaving\n",
        "\n",
        "In our employee retention problem, rather than simply predicting whether an employee will leave the company within a certain time frame, we would much rather have an estimate of the probability that he/she will leave the company. We would rank employees by their probability of leaving, then allocate a limited incentive budget to the highest probability instances.\n",
        "\n",
        "Consider employee turnover domain where an employee is given treatment by Human Resources because they think the employee will leave the company within a month, but the employee actually does not. This is a false positive. This mistake could be expensive, inconvenient, and time consuming for both the Human Resources and employee, but is a good investment for relational growth.\n",
        "\n",
        "Compare this with the opposite error, where Human Resources does not give treatment/incentives to the employees and they do leave. This is a false negative. This type of error is more detrimental because the company lost an employee, which could lead to great setbacks and more money to rehire. Depending on these errors, different costs are weighed based on the type of employee being treated. For example, if it’s a high-salary employee then would we need a costlier form of treatment? What if it’s a low-salary employee? The cost for each error is different and should be weighed accordingly.\n",
        "\n",
        "Solution 1:\n",
        "\n",
        "We can rank employees by their probability of leaving, then allocate a limited incentive budget to the highest probability instances.\n",
        "OR, we can allocate our incentive budget to the instances with the highest expected loss, for which we'll need the probability of turnover.\n",
        "\n",
        "Solution 2: Develop learning programs for managers. Then use analytics to gauge their performance and measure progress. Some advice:\n",
        "\n",
        "Be a good coach\n",
        "Empower the team and do not micromanage\n",
        "Express interest for team member success\n",
        "Have clear vision / strategy for team\n",
        "Help team with career development"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}